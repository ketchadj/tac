{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all.txt',\n",
       " 'juuu.txt',\n",
       " 'KB_JB1051_1921-10-08_01-00001.txt',\n",
       " 'KB_JB1051_1921-10-15_01-00001.txt',\n",
       " 'KB_JB1051_1921-10-15_01-00003.txt',\n",
       " 'KB_JB1051_1921-10-15_01-00004.txt',\n",
       " 'KB_JB1051_1921-10-22_01-00003.txt',\n",
       " 'KB_JB1051_1921-10-22_01-00004.txt',\n",
       " 'KB_JB1051_1921-12-03_01-00004.txt',\n",
       " 'KB_JB1051_1921-12-17_01-00004.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = '../data/drapeau'\n",
    "\n",
    "pdfs = []\n",
    "for f in os.listdir(pdf_path):\n",
    "    if os.path.isfile(os.path.join(pdf_path, f)):\n",
    "        pdfs.append(f)\n",
    "pdfs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer un répertoire TXT s'il n'existe pas encore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'txt_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m txts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43mtxt_path\u001b[49m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(txt_path, f)):\n\u001b[0;32m      4\u001b[0m         txts\u001b[38;5;241m.\u001b[39mappend(f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'txt_path' is not defined"
     ]
    }
   ],
   "source": [
    "txts = []\n",
    "for f in os.listdir(txt_path):\n",
    "    if os.path.isfile(os.path.join(txt_path, f)):\n",
    "        txts.append(f)\n",
    "txts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/drapeau/all.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for file in os.listdir(txt_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(txt_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                output_file.write(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction de Keywords : avec le corpus idependance qui correspond à independance belge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraire les mots clés d'un document avec Yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ici nous prenons idependance , pour avoir le corpus du soir et drapeau , remplaçons idependance pas celle qui convient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les Fichiers\n",
    "data_path = \"../data/idependance/\"\n",
    "files = [f for f in os.listdir(data_path) if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer le nombre de fichiers identifiés\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(year) for year in range(1831, 1840)]  # Par exemple, de 1996 à 2022\n",
    "\n",
    "for year in years:\n",
    "    # Filtrer les fichiers correspondant à l'année actuelle\n",
    "    files_for_year = [f for f in os.listdir(data_path) if year in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    # Filtrer les fichiers correspondant à l'année actuelle\n",
    "    files_for_year = [f for f in os.listdir(data_path) if year in f]\n",
    "\n",
    "    # Initialiser le texte pour l'année en cours\n",
    "    text = \"\"\n",
    "\n",
    "    # Concaténer le contenu de tous les fichiers correspondant à l'année\n",
    "    for filename in files_for_year:\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text += f.read()\n",
    "\n",
    "    # Affichage des premiers 500 caractères du texte pour chaque année\n",
    "    print(f\"Texte pour l'année {year} :\")\n",
    "    print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = kw_extractor.extract_keywords(text)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept = []\n",
    "for kw, score in keywords:\n",
    "    words = kw.split()\n",
    "    if len(words) == 2:\n",
    "        kept.append(kw)\n",
    "kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Création d'une liste pour tous les mots-clés de 1831 à 1835\n",
    "all_keywords_1831_1840 = []\n",
    "\n",
    "# Faire la même opération sur tous les documents de la période spécifiée\n",
    "for year in range(1831, 1840):\n",
    "    files_for_year = [f for f in os.listdir(data_path) if str(year) in f]\n",
    "\n",
    "    for filename in files_for_year:\n",
    "        text = open(os.path.join(data_path, filename), 'r', encoding='utf-8').read()\n",
    "        keywords = kw_extractor.extract_keywords(text)\n",
    "        kept = []\n",
    "        for kw, score in keywords:\n",
    "            words = kw.split()\n",
    "            if len(words) == 2:\n",
    "                kept.append(kw)\n",
    "                all_keywords_1831_1840.append((kw, score))\n",
    "        print(f\"{filename} mentions these keywords: {', '.join(kept)}...\")\n",
    "\n",
    "# Tri des mots-clés par score (croissant)\n",
    "all_keywords_1831_1840.sort(key=lambda x: x[1])\n",
    "\n",
    "# Écriture des mots-clés dans un fichier CSV\n",
    "with open('keywords_1831_1840.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    keyword_writer = csv.writer(csvfile, delimiter=',')\n",
    "    keyword_writer.writerow(['Keyword', 'Score'])  # Écriture de l'en-tête\n",
    "    for kw, score in all_keywords_1831_1840:\n",
    "        keyword_writer.writerow([kw, score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance d'entités nommées avec SpaCy : avec le corpus idependance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliquer la reconnaissance d'entités nommées sur notre corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le texte\n",
    "n=100000\n",
    "text = open(\"../data/keywords_1831_1840.csv\", encoding='utf-8').read()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le texte\n",
    "n=100000\n",
    "text = open(\"../data/keywords_1831_1840.csv\", encoding='utf-8').read()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Traiter le texte\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " lister  les organisations (PER) les plus mentionnées dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for person, freq in sorted_people[:50]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lister  les organisations (ORG) les plus mentionnées dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"ORG\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for person, freq in sorted_people[:50]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lister  les LOCALISATIONS (LOC) les plus mentionnées dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"LOC\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for person, freq in sorted_people[:50]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuages de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports et stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords (Idem que dans s1)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\",\"III\",\"ETC\",\"dès\",\"elles\",\"et\",\"pour\",\"seul\",\"celui\",\"alors\",\"vers\",\"effet\",\"ailleurs\",\n",
    "       \"tel\",\"deja\",\"culs\",\"chez\",\"l'\",\"d'\",\"bur'\",\"faç'\",\"mais\",\"ces\",\"leurs\",\"meme\",\"aux'\",\"sur'\",\"sans'\",\"moins\", \"cas\", \"vue\", \"toujour\", \"ceux\",\n",
    "       \"peu\",\"moment\",\"nos\",\"seul\",\"avoir\",\"été\",\"point\",\"ici\",\"quand\",\"plus\",\"fut\",\"non\",\"notre\",]\n",
    "sw = set(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer un fichier contenant le texte de tous les journaux d'une année donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les chemins d'accès\n",
    "data_path = '../data'\n",
    "txt_path = '../data/idependance'  # Ou un chemin spécifique à vos fichiers\n",
    "\n",
    "# Liste des années de 1835 à 1840\n",
    "years = [str(year) for year in range(1835, 1840)]  # 1953 exclu pour inclure 1952\n",
    "\n",
    "# Réaliser le processus pour chaque année\n",
    "for year in years:\n",
    "    # Stocker le contenu des fichiers dans une liste\n",
    "    content_list = []\n",
    "    for txt in os.listdir(txt_path):\n",
    "        if year in txt:\n",
    "            with open(os.path.join(txt_path, txt), 'r', encoding='utf-8') as f:\n",
    "                content_list.append(f.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrire tout le contenu dans un fichier temporaire\n",
    "temp_path = f'../data/tmp_{year}'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "with open(os.path.join(temp_path, f'{year}.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour nettoyer le texte\n",
    "def clean_text(year, folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"{year}.txt\"\n",
    "        output_path = f\"{year}_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/{year}.txt\"\n",
    "        output_path = f\"{folder}/{year}_clean.txt\"\n",
    "    \n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        sw = set(nltk.corpus.stopwords.words('english'))  # Ajoutez les stopwords appropriés ici\n",
    "        kept = [w.upper() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'\n",
    "\n",
    "# Nettoyer le texte pour l'année donnée\n",
    "clean_text(year, folder=temp_path)\n",
    "# Vérifier le résultat après le nettoyage\n",
    "with open(os.path.join(temp_path, f'{year}_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "# Afficher les 500 premiers caractères après nettoyage\n",
    "print(after[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre d'éléments (=fichiers) dans la liste\n",
    "len(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire\n",
    "temp_path = '../data/tmp'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "with open(os.path.join(temp_path, f'{year}.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(content_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer le fichier à l'aide d'une fonction de nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les fréquences des mots\n",
    "frequencies = Counter(after.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer, stocker et afficher le nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=2000, height=1000, background_color='white').generate_from_frequencies(frequencies)\n",
    "cloud.to_file(os.path.join(temp_path, f\"{year}.png\"))\n",
    "Image(filename=os.path.join(temp_path, f\"{year}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis \n",
    "\n",
    "## 1. Textblob-FR\n",
    "\n",
    "Documentation: https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une fonction `get_sentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyser les sentiments des phrases suivantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"je demande à êlre jugé, s'il y a possibilité de me juger; si la possibilité de méjuger n'existe plus, déclarez-le\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Partant de là , ou doit donner au mot de domination le sens le plus large , el reconnailre que la Belgique toule entière a de droit changé défini ivemenl de domination aux yeux des puissances qui l'ont reconnue, aux yeux du Roi Guillaume, qui a abdiqué , quant à la Belgique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Nous soutenons, au conlraire, que l'amnistie doit recevoir l'application la plus générale ; qne l'amnistie doit profiter à tous ceux qui , soil dans le Limbourg belge, ou le Limbourg hollandais, soit dans le Luxembourg allemand, soit dans le Luxembourg belge , soit enfin dans la Belgique entière, se irouveraienl dans le cas d'être inquiétés pour une parlicipation quelconque aux fails de la révolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"l est bien certain que s'il y avait eu dans ce fait récompense , il n'y aurail pas eu encouragement ponr la discipline ; mais cela n'est nullement exact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"En effet, pour quiconque veut lire l'article 20 avec impartialité, sans préoccupation, sans nécessité de falsifier son sens, ce sens est de loule évidence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"je vous signalais dernièrement la rréàiion du conseil dé l'EmpixejReichsrelhi, fà- ranti par la Charte et qui vient d'être organisé comme une mesure de la plus haute importance, si, Oh effet, on y procède d'après la lettre et l'esprit dé la Charte qui dit, art. 96 el 97 : « Qu'à côlé de la couronne et du pouvoir exécutif, il est institué un conseil do l'Empire ayant voix consultative stir toutes les aîalr'es sur lesquelles le pouvoir exécutif réclamera son avis ; que les membres de ce conseil sont nommés par l'Empereur; et qde dans leur nomination on doit, autant que possible, prendre en considération les diverses parties dé i'empifè (les diverses nationalités) » Par là on arrivera d'autant plus aisément à réunir les plus grandes capacités dans l'administration, la politique et la jurisprudence, que leur position dans le conseil de l'Empire n'est pas comme celle du ministère responsable et ne représente pas une certaine couleur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Je profite de la trêve que le dimanche vient d'apporter aux débats de la semaine, pour ajouter encore quelques mots aux détails que je vous ai transmis dans ma lettre précédente sur la discussion générale du budget. Les débats ont eu, comme dej-aison, un caractère double Ils ont roulé en même temps sur les mérites du budget proposé et sur la politique générale du ministère.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Mais je.crois que la 'Chambre poserait un précédent fàoheux en l'admttânt à voter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"L’équipe anversoise est arrivée A ses fins, aprè*. avoir mené le match quasi de bout en bout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"“Van Kersschaever. au pivot, n’atteignit Jamais son rendement habituel Alors qu’il était considéré comme l’atout majeur de l’équipe ostendnise, il se révéla franchement décevant et il ne tira guère profit, hormis en récupération défensive, de son avantage de taille\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ici on utilise le corpus drapeau c'est pareil pour le soir et independance belge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/le soir/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choisir une periode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_1939_to_1945 = [str(year) for year in range(1939, 1945)]  # Liste des années de 1831 à 1835\n",
    "\n",
    "articles_1939_to_1945 = [f for f in os.listdir(data_path) if any(year in f for year in years_1939_to_1945)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger tous les  fichiers de la décennie et en créer une liste de textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_1939_to_1945 = [str(year) for year in range(1939, 1945)]  # Liste des années de 1831 à 1835\n",
    "\n",
    "articles_1939_to_1945 = [\n",
    "    open(os.path.join(data_path, f), \"r\", encoding=\"utf-8\").read() \n",
    "    for f in os.listdir(data_path) \n",
    "    if any(year in f for year in years_1939_to_1945)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoriser les documents à l'aide de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire la matrice de vecteurs à l'aide de la fonction `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = vectorizer.fit_transform(articles_1939_to_1945)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur tf-IDF du premier document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendre les vecteurs et leurs \"distances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définir un nombre de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquer le clustering à l'aide de la fonction `fit_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(data_path)  # Obtenez la liste de tous les fichiers dans le répertoire\n",
    "\n",
    "# Assurez-vous de la correspondance entre les fichiers et les vecteurs TF-IDF\n",
    "# Cela suppose que les fichiers sont dans le même ordre que les vecteurs TF-IDF ont été créés\n",
    "clustering = collections.defaultdict(list)\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser les clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Générer le plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "silhouette score moyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Supposons que reduced_vectors est votre ensemble de données réduites après PCA, et clusters contient les labels de clusters attribués à chaque point\n",
    "\n",
    "# Calcul du score de silhouette\n",
    "silhouette_avg = silhouette_score(reduced_vectors, clusters)\n",
    "print(f\"Silhouette Score moyen : {silhouette_avg}\")\n",
    "\n",
    "# Visualisation des données réduites avec les clusters et les centroïdes\n",
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarité moyenne intra-cluster pour le cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_texts = {}\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Supposons que 'clustered_texts' est une liste de clusters où chaque cluster contient des documents similaires\n",
    "articles_1939_to_1945 = [\n",
    "    open(os.path.join(data_path, f), \"r\", encoding=\"utf-8\").read() \n",
    "    for f in os.listdir(data_path) \n",
    "    if any(year in f for year in years_1939_to_1945)\n",
    "]\n",
    "\n",
    "# Utilisez ces articles pour créer la variable all_articles\n",
    "all_articles = articles_1939_to_1945\n",
    "# Création des clusters et stockage des textes associés\n",
    "for idx, cluster_idx in enumerate(clusters):\n",
    "    if cluster_idx not in clustered_texts:\n",
    "        clustered_texts[cluster_idx] = []\n",
    "    clustered_texts[cluster_idx].append(all_articles[idx])\n",
    "\n",
    "# Calcul de la similarité intra-cluster pour chaque cluster\n",
    "for cluster_idx, cluster_text in clustered_texts.items():\n",
    "    # Convertir le cluster de texte en une matrice TF-IDF\n",
    "    tfidf_matrix_cluster = vectorizer.fit_transform(cluster_text)\n",
    "\n",
    "    # Calculer la similarité entre les documents dans le cluster\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix_cluster, tfidf_matrix_cluster)\n",
    "    mean_similarity = similarity_matrix.mean()  # Similarité moyenne dans le cluster\n",
    "    print(f\"Similarité moyenne intra-cluster pour le cluster {cluster_idx} : {mean_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rom sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Supposons que 'clustered_texts' est une liste de clusters où chaque cluster contient des documents similaires\n",
    "for cluster_idx, cluster_text in clustered_texts.items():\n",
    "    # Convertir le cluster de texte en une matrice TF-IDF (exemple)\n",
    "    tfidf_vectorizer_cluster = TfidfVectorizer()\n",
    "    tfidf_matrix_cluster = tfidf_vectorizer_cluster.fit_transform(cluster_text)\n",
    "\n",
    "    # Calculer la similarité entre les documents dans le cluster\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix_cluster, tfidf_matrix_cluster)\n",
    "    mean_similarity = similarity_matrix.mean()  # Similarité moyenne dans le cluster\n",
    "    print(f\"Similarité moyenne intra-cluster pour le cluster {cluster_idx} : {mean_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings : le modèle Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et traitement des phrases du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dans ce code c'est le soir qui est utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f\"../data/le soir/sen (2).txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection des bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bigram_phrases.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigram_phrases.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ = list(bigram_phrases.vocab.keys())[144]\n",
    "print(key_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases.vocab[key_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion des `Phrases` en objet `Phraser`\n",
    "\n",
    "`Phraser` est un alias pour `gensim.models.phrases.FrozenPhrases`, voir ici https://radimrehurek.com/gensim/models/phrases.html.\n",
    "\n",
    "Le `Phraser` est une version *light* du `Phrases`, plus optimale pour transformer les phrases en concaténant les bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un corpus d'unigrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement d'un modèle Word2Vec sur ce corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=5, # La taille du \"contexte\", ici 5 mots avant et après le mot observé\n",
    "    min_count=5, # On ignore les mots qui n'apparaissent pas au moins 5 fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en 4 threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger le modèle en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"../data/newspapers.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer la similarité entre deux termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"gouvernement\", \"politique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"revolution\", \"syndicat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"communisme\", \"socialisme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"diplomatie\", \"commission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"guerre\", \"manifestation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
